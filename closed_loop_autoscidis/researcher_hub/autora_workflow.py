"""
Basic Workflow
    Two Independent Variables, One Dependent Variable
    Theorist: Logistic Regression, Bayesian Machine Scientist
    Experimentalist: Random Sampling, Model Disagreement
    Runner: Firebase Runner (no prolific recruitment)
"""

import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from autora.variable import VariableCollection, Variable
from autora.theorist.bms import BMSRegressor
from autora.experimentalist.grid import grid_pool
from autora.experimentalist.random import random_sample
from autora.experimentalist.model_disagreement import model_disagreement_sample
from autora.experiment_runner.firebase_prolific import firebase_runner
from autora.state import StandardState, on_state, Delta

from sklearn.linear_model import LogisticRegression
from sklearn.base import BaseEstimator, ClassifierMixin

from trial_sequence import trial_sequence
from stimulus_sequence import stimulus_sequence
from preprocessing import trial_list_to_experiment_data


# ⬇⬇⬇ NEU: direkt unter deinen anderen Imports einfügen
try:
    from autora.theorist.nuts import NutsRegressor  # bevorzugter Exportname
except ImportError:
    # falls du nur NutsTheorists exportierst:
    from autora.theorist.nuts import NutsTheorists as NutsRegressor
# ⬆⬆⬆ NEU


# Some study parameters
num_cycles = 2
num_trials = 10
num_conditions_per_cycle = 2

# *** Set up variables *** #
# The independent variables correspond to dot numbers for left and right stimulus.
# The dependent variable is accuracy ranging from 0 (0 percent) to 1 (100 percent)
variables = VariableCollection(
    independent_variables=[
        Variable(name="dots_left", allowed_values=np.linspace(1, 100, 100)),
        Variable(name="dots_right", allowed_values=np.linspace(1, 100, 100)),
        ],
    dependent_variables=[Variable(name="accuracy", value_range=(0, 1))])

# Here, we enlist the entire design space, by combining all possibles values 
# of each of the two independent variables
allowed_conditions = grid_pool(variables)
# We remove the conditions where the number of dots is the same on both sides
allowed_conditions = allowed_conditions[allowed_conditions['dots_left'] != allowed_conditions['dots_right']]

# *** State *** #
# With the variables, we can set up a state. The state object represents the state of our
# closed-loop experiment.
state = StandardState(
    variables=variables,
)

# *** Components/Agents *** #
# Components are functions that run on the state. The main components are:
# - theorist
# - experiment-runner
# - experimentalist
# Learn more about components here: 
# https://autoresearch.github.io/autora/tutorials/basic/Tutorial%20I%20Components/


# ** Theorist ** #
# Here we use two different theorists:
# - Logistic Regression for discovering a linear regression model
# - Bayesian Machine Scientist for discovering a (non-linear) model
# Later, we will compare the models generated by both theorists 
# For more information about AutoRA theorists, see 
# https://autoresearch.github.io/autora/theorist/

# First, we define a custom LogisticRegressor class that inherits from BaseEstimator and ClassifierMixin
# We override the predict method to return the probability of the '1' (accurate) class,
# to align its output with that of the BMSRegressor
class LogisticRegressor(BaseEstimator, ClassifierMixin):
    def __init__(self, *args, **kwargs):
        self.model = LogisticRegression(*args, **kwargs)  # Initialize the LogisticRegression model with any passed arguments

    def fit(self, X, y):
        """Fit the LogisticRegression model."""
        self.model.fit(X, y)
        return self

    def predict(self, X):
        """Override the predict method to return the probability of the '1' (accurate) class."""
        return self.model.predict_proba(X)[:, 1].reshape(-1, 1) 

# Initialize the BMSRegressor and LogisticRegressor
bms_theorist = BMSRegressor(epochs=500)
lr_theorist = LogisticRegressor()
nuts_theorist = NutsRegressor()



# To use the theorists on the state object, we wrap it with the on_state functionality and return a
# Delta object, indicating how we want to modify the state.
# Note: The if the input arguments of the theorist_on_state function are state-fields like
# experiment_data, variables, ..., then calling this function on a state object will automatically
# update those state fields. You can learn more about states and the on_state function here:
# https://autoresearch.github.io/autora/tutorials/basic/Tutorial%20IV%20Customization/

@on_state()
def theorist_on_state(experiment_data, variables):
    ivs = [iv.name for iv in variables.independent_variables]
    dvs = [dv.name for dv in variables.dependent_variables]
    x = experiment_data[ivs]
    y = experiment_data[dvs]
    return Delta(models=[
        nuts_theorist.fit(x, y),   # dein Modell
        bms_theorist.fit(x, y),
        lr_theorist.fit(x, y),
    ])



# ** Experimentalists ** #
# We will seed our study with randomly sampled experiment conditions, 
# using a random pool experimentalist. Here, we use a random pool and use the wrapper
# to that operates on the state object.
@on_state()
def initialize_state(allowed_conditions, num_samples):
    return Delta(conditions=random_sample(allowed_conditions, num_samples))

# After collecting our first data, we will use a model disagreement experimentalist
# to determine the next experiment conditions.
@on_state()
def experimentalist_on_state(allowed_conditions, models_to_compare, num_samples):
    return Delta(conditions=model_disagreement_sample(allowed_conditions, models_to_compare, num_samples))


# ** Experiment Runner ** #
# We will run our experiment on firebase and need credentials. You will find them here:
# (https://console.firebase.google.com/)
#   -> project -> project settings -> service accounts -> generate new private key

# in closed_loop_autoscidis/researcher_hub/autora_workflow.py
import json, pathlib
CRED_PATH = pathlib.Path(__file__).with_name("firebase-service-account.json")
firebase_credentials = json.loads(CRED_PATH.read_text(encoding="utf-8"))



# Simple experiment runner that runs the experiment on firebase
# The runner defines a timeout of 100 seconds, which means that a participant
# has 5 *minutes* to complete an experiment. Afterward, it will be freed for another participant.
# The sleep time is set to 3 *seconds*, which means that the runner will check every 5 seconds for data.
experiment_runner = firebase_runner(
    firebase_credentials=firebase_credentials,
    time_out=5,
    sleep_time=3)

# Again, we need to wrap the experiment runner to use it on the state.
# Specifically, the runner compiles the identified conditions (i.e., number of tested dots)
# into an experiment sequence, which is then used to generate the full JavaScript experiment
# to be sent to Firebase. Once the data are collected, the runner will then pre-process it 
# and store it in the experimental_data object
@on_state()
def runner_on_state(conditions):
    res = []
    for idx, c in conditions.iterrows():
        iv_1 = c['dots_left']
        iv_2 = c['dots_right']
        # get a timeline via sweetPea
        timeline = trial_sequence(iv_1, iv_2, num_trials)
        print("Generated counterbalanced trial sequence.")
        # get js code via sweetBeaan
        js_code = stimulus_sequence(timeline)
        print("Compiled experiment.")
        res.append(js_code)

    # prepare conditions to send
    conditions_to_send = conditions.copy()
    conditions_to_send['experiment_code'] = res
    # upload and run the experiment:
    print("Uploading the experiment...")
    data_raw = experiment_runner(conditions_to_send)
    print("Collected experimental data.")

    # process the experiment data
    experiment_data = pd.DataFrame()
    for item in data_raw:
        _lst = json.loads(item)['trials']
        _df = trial_list_to_experiment_data(_lst)
        experiment_data = pd.concat([experiment_data, _df], axis=0)
    print("Preprocessed experimental data.")
    return Delta(experiment_data=experiment_data)


# *** AutoRA Workflow *** #
# Next, we specify our actual workflow using the components defined above.
# We begin with sampling two random initial conditions for the dots, resulting in two experiments.
# We then iterate three times through the following steps:
# 1) Collect data.
# 2) Fit models.
# 3) Identify novel experiment conditions.

state = initialize_state(state, allowed_conditions=allowed_conditions, num_samples=num_conditions_per_cycle)

# Now, we can run our components in a loop
for _ in range(num_cycles):
    state = runner_on_state(state)
    print("Finished data collection and preprocessing.")
    state = theorist_on_state(state)
    print("Fitted models.")
    models_to_compare = [state.models[-1], state.models[-2], state.models[-3]]

    state = experimentalist_on_state(state, 
                                     allowed_conditions=allowed_conditions, 
                                     models_to_compare=models_to_compare, 
                                     num_samples=num_conditions_per_cycle)  
    print("Determined experiment conditions.")


# *** Plot *** #
# This plot visualizes the fit of the final logistic regression model and the model identified by the
# Bayesian Machine Scientist. 
ivs = [iv.name for iv in variables.independent_variables]
dvs = [dv.name for dv in variables.dependent_variables]
X = state.experiment_data[ivs]
y = state.experiment_data[dvs]

# Create a meshgrid for plotting the logistic regression decision boundary
iv1_range = variables.independent_variables[0].allowed_values
iv2_range = variables.independent_variables[1].allowed_values
iv1_grid, iv2_grid = np.meshgrid(iv1_range, iv2_range)
iv_grid = np.c_[iv1_grid.ravel(), iv2_grid.ravel()]

# Predict probabilities for the meshgrid (for the surface plot)
dv_pred_lr = state.models[-1].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class
dv_pred_bms = state.models[-2].predict(iv_grid).reshape(iv1_grid.shape)  # Probability of the positive class
dv_pred_nuts = state.models[-3].predict(iv_grid).reshape(iv1_grid.shape)


# Create the 3D plot
fig = plt.figure(figsize=(14, 6))

ax1 = fig.add_subplot(121, projection='3d')
# Plot the actual data points
ax1.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')
# Plot the logistic regression surface
ax1.plot_surface(iv1_grid, iv2_grid, dv_pred_lr, cmap='viridis', alpha=0.6)
# Label the axes
ax1.set_xlabel('Number of Dots (left)')
ax1.set_ylabel('Number of Dots (right)')
ax1.set_zlabel('Accuracy')
ax1.set_zlim(0, 1)
ax1.set_title("Logistic Regression")

ax2 = fig.add_subplot(122, projection='3d')
# Plot the actual data points
ax2.scatter(X['dots_left'], X['dots_right'], y, color='red', label='Data Points')
# Plot the logistic regression surface
ax2.plot_surface(iv1_grid, iv2_grid, dv_pred_bms, cmap='viridis', alpha=0.6)
# Label the axes
ax2.set_xlabel('Number of Dots (left)')
ax2.set_ylabel('Number of Dots (right)')
ax2.set_zlabel('Accuracy')
ax2.set_zlim(0, 1)
ax2.set_title("BMS Equation: " + state.models[-2].repr())

# Show the plot
plt.savefig('model_comparison.png')
plt.show()